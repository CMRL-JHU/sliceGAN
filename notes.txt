brayan mentioned discriminator not going to dim of 1 when extra channels added

why do we do the mean of the data?
	out_real = netD(data_real).view(-1).mean()
	this just takes all of the channels (4 for quats, 5 for quats and featureids) and shoves them together
	I think this should be fixed with a linear module in the discriminator neural network

why is there a difference?
	shape_disc =           [l * D_batch_size, nc, l, l]
    shape_gen  = [          l *   batch_size, nc, l, l]
	alpha      = alpha.view(      batch_size, nc, l, l)
		recently changed to:
			alpha      = alpha.view(-1, nc, l, l)
	
why are we throwing away some of the data that goes to the gradient penalty?
	gradient_penalty = util.calc_gradient_penalty(netD, data_real, data_fake[:batch_size],
																		  batch_size, l,
																		  device, Lambda, nc)
																		  

3d, batch size 4, nc 4
noise shape:  torch.Size([1, 16, 4, 4, 4])
fake data shape:  torch.Size([1, 4, 64, 64, 64])
real data shape torch.Size([4, 4, 64, 64])
real data discriminator output:  tensor([-0.0580], grad_fn=<AddBackward0>)
fake data permuted:  torch.Size([64, 4, 64, 64])

3d, batch size 2, nc 4
noise shape:  torch.Size([0, 16, 4, 4, 4])
fake data shape:  torch.Size([0, 4, 64, 64, 64])
real data shape: torch.Size([2, 4, 64, 64])
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x2 and 4x1)